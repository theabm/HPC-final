\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{none/global//global/global}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Benchmarking MKL, OpenBLAS, and BLIS}{2}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{2}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Methodology}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Compiling BLIS and obtaining binaries}{2}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Using a fixed number of cores}{3}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Using a fixed matrix size}{3}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Results and Discussion}{3}{section.1.3}\protected@file@percent }
\newlabel{eq:perf}{{1.1}{3}{Results and Discussion}{equation.1.3.1}{}}
\abx@aux@cite{0}{arch}
\abx@aux@segm{0}{0}{arch}
\abx@aux@cite{0}{arch}
\abx@aux@segm{0}{0}{arch}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Performance table of THIN and EPYC node architectures.}}{4}{table.1.1}\protected@file@percent }
\newlabel{tab:perf}{{1.1}{4}{Performance table of THIN and EPYC node architectures}{table.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Using a fixed number of cores}{4}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{THIN Nodes}{4}{section*.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  Results of SP matrix-matrix multiplication for THIN nodes. \texttt  {MKL} and \texttt  {OpenBLAS} perform similarly, outperforming \texttt  {BLIS} for all matrix sizes.}}{4}{figure.1.1}\protected@file@percent }
\newlabel{fig:fixed_cores_thin_float}{{1.1}{4}{Results of SP matrix-matrix multiplication for THIN nodes. \texttt {MKL} and \texttt {OpenBLAS} perform similarly, outperforming \texttt {BLIS} for all matrix sizes}{figure.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces  Results of DP matrix-matrix multiplication for THIN nodes. \texttt  {MKL} performs the best, slightly above \texttt  {OpenBLAS}. Both outperform \texttt  {BLIS} for all matrix sizes.}}{5}{figure.1.2}\protected@file@percent }
\newlabel{fig:fixed_cores_thin_double}{{1.2}{5}{Results of DP matrix-matrix multiplication for THIN nodes. \texttt {MKL} performs the best, slightly above \texttt {OpenBLAS}. Both outperform \texttt {BLIS} for all matrix sizes}{figure.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{EPYC Nodes}{5}{section*.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces  Results of SP matrix-matrix multiplication for EPYC nodes. We notice that asymptotically, \texttt  {OpenBLAS} outperforms \texttt  {MKL} and \texttt  {BLIS}, while for small matrices, \texttt  {OpenBLAS} performs the worst.}}{6}{figure.1.3}\protected@file@percent }
\newlabel{fig:fixed_cores_epyc_float}{{1.3}{6}{Results of SP matrix-matrix multiplication for EPYC nodes. We notice that asymptotically, \texttt {OpenBLAS} outperforms \texttt {MKL} and \texttt {BLIS}, while for small matrices, \texttt {OpenBLAS} performs the worst}{figure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces  Results of DP matrix-matrix multiplication for EPYC nodes. \texttt  {BLIS} outperforms \texttt  {MKL} and \texttt  {OpenBLAS} for all matrix sizes.}}{6}{figure.1.4}\protected@file@percent }
\newlabel{fig:fixed_cores_epyc_double}{{1.4}{6}{Results of DP matrix-matrix multiplication for EPYC nodes. \texttt {BLIS} outperforms \texttt {MKL} and \texttt {OpenBLAS} for all matrix sizes}{figure.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Using a fixed matrix size}{7}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{THIN Nodes}{7}{section*.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces  Results of SP matrix-matrix multiplication as the number of cores increase, using OMP close policy. \texttt  {MKL} and \texttt  {OpenBLAS} obtain the best performance.}}{7}{figure.1.5}\protected@file@percent }
\newlabel{fig:fixed_size_thin_float_close}{{1.5}{7}{Results of SP matrix-matrix multiplication as the number of cores increase, using OMP close policy. \texttt {MKL} and \texttt {OpenBLAS} obtain the best performance}{figure.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces  Results of SP matrix-matrix multiplication as the number of cores increase, using OMP spread policy. \texttt  {MKL} and \texttt  {OpenBLAS} are the best perfomers.}}{8}{figure.1.6}\protected@file@percent }
\newlabel{fig:fixed_size_thin_float_spread}{{1.6}{8}{Results of SP matrix-matrix multiplication as the number of cores increase, using OMP spread policy. \texttt {MKL} and \texttt {OpenBLAS} are the best perfomers}{figure.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces  Results of DP matrix-matrix multiplication as the number of cores increase, using close policy. \texttt  {MKL} and \texttt  {OpenBLAS} perform the best.}}{8}{figure.1.7}\protected@file@percent }
\newlabel{fig:fixed_size_thin_double_close}{{1.7}{8}{Results of DP matrix-matrix multiplication as the number of cores increase, using close policy. \texttt {MKL} and \texttt {OpenBLAS} perform the best}{figure.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces  Results of DP matrix-matrix multiplication as the number of cores increase, using spread policy.}}{9}{figure.1.8}\protected@file@percent }
\newlabel{fig:fixed_size_thin_double_spread}{{1.8}{9}{Results of DP matrix-matrix multiplication as the number of cores increase, using spread policy}{figure.1.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{EPYC Nodes}{9}{section*.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces  Results of SP matrix-matrix multiplication as the number of cores increase, using close policy. The best performance is obtained by \texttt  {OpenBLAS} with 40 cores.}}{9}{figure.1.9}\protected@file@percent }
\newlabel{fig:fixed_size_epyc_float_close}{{1.9}{9}{Results of SP matrix-matrix multiplication as the number of cores increase, using close policy. The best performance is obtained by \texttt {OpenBLAS} with 40 cores}{figure.1.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces  Results of SP matrix-matrix multiplication as the number of cores increase, using spread policy.}}{10}{figure.1.10}\protected@file@percent }
\newlabel{fig:fixed_size_epyc_float_spread}{{1.10}{10}{Results of SP matrix-matrix multiplication as the number of cores increase, using spread policy}{figure.1.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces  Results of DP matrix-matrix multiplication as the number of cores increase, using close policy. The best performance is obtained by \texttt  {BLIS}, followed by \texttt  {OpenBLAS}.}}{10}{figure.1.11}\protected@file@percent }
\newlabel{fig:fixed_size_epyc_double_close}{{1.11}{10}{Results of DP matrix-matrix multiplication as the number of cores increase, using close policy. The best performance is obtained by \texttt {BLIS}, followed by \texttt {OpenBLAS}}{figure.1.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces  Results of DP matrix-matrix multiplication as the number of cores increase, using spread policy.}}{11}{figure.1.12}\protected@file@percent }
\newlabel{fig:fixed_size_epyc_double_spread}{{1.12}{11}{Results of DP matrix-matrix multiplication as the number of cores increase, using spread policy}{figure.1.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Conclusions}{11}{section.1.4}\protected@file@percent }
\abx@aux@cite{0}{conway}
\abx@aux@segm{0}{0}{conway}
\abx@aux@cite{0}{conway_patterns}
\abx@aux@segm{0}{0}{conway_patterns}
\abx@aux@cite{0}{pgm}
\abx@aux@segm{0}{0}{pgm}
\abx@aux@cite{0}{mpi}
\abx@aux@segm{0}{0}{mpi}
\abx@aux@cite{0}{omp}
\abx@aux@segm{0}{0}{omp}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Conway's Game of Life}{12}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{12}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Methodology}{13}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Decomposition}{13}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces  Decomposition of an array among three processes. Although the array is conceptualized as a 2D structure, internally, for efficiency, it is represented as a 1D contigous block of memory.}}{13}{figure.2.1}\protected@file@percent }
\newlabel{fig:decomposition}{{2.1}{13}{Decomposition of an array among three processes. Although the array is conceptualized as a 2D structure, internally, for efficiency, it is represented as a 1D contigous block of memory}{figure.2.1}{}}
\abx@aux@cite{0}{prace}
\abx@aux@segm{0}{0}{prace}
\abx@aux@cite{0}{prace}
\abx@aux@segm{0}{0}{prace}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  2D decomposition of a grid. As we can see, each process needs to deal with non-contiguous memory. Image courtesy of \blx@tocontentsinit {0}\cite {prace}.}}{14}{figure.2.2}\protected@file@percent }
\newlabel{fig:2ddecomposition}{{2.2}{14}{2D decomposition of a grid. As we can see, each process needs to deal with non-contiguous memory. Image courtesy of \cite {prace}}{figure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces  On the figure in the left, we can see that we can process cells that are internal with no problem. However, at the boundary we need information that belongs to another process. Therefore, we allocate some extra space and do a halo exchange as shown in the figure on the right.}}{14}{figure.2.3}\protected@file@percent }
\newlabel{fig:haloexchange}{{2.3}{14}{On the figure in the left, we can see that we can process cells that are internal with no problem. However, at the boundary we need information that belongs to another process. Therefore, we allocate some extra space and do a halo exchange as shown in the figure on the right}{figure.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}MPI IO}{15}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Mixing MPI and OMP}{15}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Implementation}{15}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}State representation}{15}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Distributing the grid among MPI processes}{16}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Allocating data}{16}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}PGM Files - Header}{16}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}MPI IO}{17}{subsection.2.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Updating Cells - Static Evolution}{18}{subsection.2.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.7}Updating Cells - Ordered Evolution}{20}{subsection.2.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.8}OMP Integration - Hybrid V1}{23}{subsection.2.3.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  Visualization of MPI Multiple mode. Each OMP thread can make MPI calls.}}{23}{figure.2.4}\protected@file@percent }
\newlabel{fig:mpi_multiple}{{2.4}{23}{Visualization of MPI Multiple mode. Each OMP thread can make MPI calls}{figure.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.9}OMP Integration - Hybrid V2}{25}{subsection.2.3.9}\protected@file@percent }
\abx@aux@cite{0}{glidergun}
\abx@aux@segm{0}{0}{glidergun}
\abx@aux@cite{0}{snarkloop}
\abx@aux@segm{0}{0}{snarkloop}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.10}Software Stack - Running the code}{26}{subsection.2.3.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.11}Miscellaneous}{26}{subsection.2.3.11}\protected@file@percent }
\abx@aux@cite{0}{MPIIOtalk}
\abx@aux@segm{0}{0}{MPIIOtalk}
\abx@aux@cite{0}{conway_implementation}
\abx@aux@segm{0}{0}{conway_implementation}
\abx@aux@cite{0}{conway_slides}
\abx@aux@segm{0}{0}{conway_slides}
\abx@aux@cite{0}{algo2}
\abx@aux@segm{0}{0}{algo2}
\abx@aux@cite{0}{algo2}
\abx@aux@segm{0}{0}{algo2}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Optional - Algorithm 2}{27}{section.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  The cell representation we use for this algorithm. }}{27}{figure.2.5}\protected@file@percent }
\newlabel{fig:mpi_multiple}{{2.5}{27}{The cell representation we use for this algorithm}{figure.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Algorithm 2 - Serial Implementation}{28}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Algorithm 2 - MPI Implementation}{31}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Algorithm 2 - OMP version}{33}{subsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Results and Discussion}{34}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Ordered Evolution}{34}{subsection.2.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces  Time of \texttt  {ordered} evolution of a random grid of size $10k$ as we increase the number of MPI processes on a THIN node.}}{34}{figure.2.6}\protected@file@percent }
\newlabel{fig:strongomp10kspeedupthin}{{2.6}{34}{Time of \texttt {ordered} evolution of a random grid of size $10k$ as we increase the number of MPI processes on a THIN node}{figure.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Static Evolution}{34}{subsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Strong OMP Scalability}{34}{section*.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces  Time for THIN nodes with a random grid of size $10k \times 10k$. V1 takes almost three times as long as V2.}}{35}{figure.2.7}\protected@file@percent }
\newlabel{fig:strongomp10kspeedupthin}{{2.7}{35}{Time for THIN nodes with a random grid of size $10k \times 10k$. V1 takes almost three times as long as V2}{figure.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces  Time for THIN nodes for V2 with increasingly bigger grids.}}{35}{figure.2.8}\protected@file@percent }
\newlabel{fig:strongompv2thin}{{2.8}{35}{Time for THIN nodes for V2 with increasingly bigger grids}{figure.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces  Speedup for strong OMP scalability using increasingly bigger grid sizes for V2. As we can see, V1 scales poorly due to the barriers that were introduced. V1 scales very well, indicating that our code is well implemented.}}{36}{figure.2.9}\protected@file@percent }
\newlabel{fig:strongompspeedupthin}{{2.9}{36}{Speedup for strong OMP scalability using increasingly bigger grid sizes for V2. As we can see, V1 scales poorly due to the barriers that were introduced. V1 scales very well, indicating that our code is well implemented}{figure.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces  Time for EPYC nodes using a random grid of size $10k$ and both \texttt  {OMP\_PROC\_BIND=close} and \texttt  {OMP\_PROC\_BIND=spread}. Results for both V1 and V2 are shown.}}{37}{figure.2.10}\protected@file@percent }
\newlabel{fig:strongomp10kepyc}{{2.10}{37}{Time for EPYC nodes using a random grid of size $10k$ and both \texttt {OMP\_PROC\_BIND=close} and \texttt {OMP\_PROC\_BIND=spread}. Results for both V1 and V2 are shown}{figure.2.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Speedup for strong OMP scalability using a random grid of $10k$ with \texttt  {OMP\_PROC\_BIND=close} and \texttt  {OMP\_PROC\_BIND=spread}. Results for both V1 and V2 are shown.}}{37}{figure.2.11}\protected@file@percent }
\newlabel{fig:strongomp10kspeedupepyc}{{2.11}{37}{Speedup for strong OMP scalability using a random grid of $10k$ with \texttt {OMP\_PROC\_BIND=close} and \texttt {OMP\_PROC\_BIND=spread}. Results for both V1 and V2 are shown}{figure.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces  Time for EPYC nodes using a random grid of size $50k$ and both \texttt  {OMP\_PROC\_BIND=close} and \texttt  {OMP\_PROC\_BIND=spread}. Only measurements for V2 were taken.}}{38}{figure.2.12}\protected@file@percent }
\newlabel{fig:strongomp10kepyc}{{2.12}{38}{Time for EPYC nodes using a random grid of size $50k$ and both \texttt {OMP\_PROC\_BIND=close} and \texttt {OMP\_PROC\_BIND=spread}. Only measurements for V2 were taken}{figure.2.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Speedup for strong OMP scalability using a random grid of $50k$ with \texttt  {OMP\_PROC\_BIND=close} and \texttt  {OMP\_PROC\_BIND=spread}. }}{39}{figure.2.13}\protected@file@percent }
\newlabel{fig:strongomp10kspeedupepyc}{{2.13}{39}{Speedup for strong OMP scalability using a random grid of $50k$ with \texttt {OMP\_PROC\_BIND=close} and \texttt {OMP\_PROC\_BIND=spread}}{figure.2.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{Strong MPI Scalability}{39}{section*.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces  Time for THIN nodes using random grids of size $50k$, $70k$, and $100k$ using 3 nodes and one thread per MPI task. }}{40}{figure.2.14}\protected@file@percent }
\newlabel{fig:strongmpithinhybrid}{{2.14}{40}{Time for THIN nodes using random grids of size $50k$, $70k$, and $100k$ using 3 nodes and one thread per MPI task}{figure.2.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces  Speedup for strong MPI scalability for random grids of size $50k$, $70k$, and $100k$ across 3 nodes. The scalability is superlinear.}}{40}{figure.2.15}\protected@file@percent }
\newlabel{fig:strongmpithinhybridspeedup}{{2.15}{40}{Speedup for strong MPI scalability for random grids of size $50k$, $70k$, and $100k$ across 3 nodes. The scalability is superlinear}{figure.2.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces  Time for EPYC nodes using random grids of size $50k$, $70k$, and $100k$ using 4 nodes and one thread per MPI task. }}{41}{figure.2.16}\protected@file@percent }
\newlabel{fig:strongmpiepychybrid}{{2.16}{41}{Time for EPYC nodes using random grids of size $50k$, $70k$, and $100k$ using 4 nodes and one thread per MPI task}{figure.2.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces  Speedup for strong MPI scalability for random grids of size $50k$, $70k$, and $100k$ across 4 nodes. The practically linear across all grid sizes.}}{42}{figure.2.17}\protected@file@percent }
\newlabel{fig:strongmpiepychybridspeedup}{{2.17}{42}{Speedup for strong MPI scalability for random grids of size $50k$, $70k$, and $100k$ across 4 nodes. The practically linear across all grid sizes}{figure.2.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{Weak MPI Scalability}{42}{section*.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces The grid dimensions we used as we increased the number of MPI tasks. Since we are working with squared grids only, it is difficult to obtain exactly the same workload. However, as we can see, the workload remains the same up to a negligible difference.}}{43}{table.2.1}\protected@file@percent }
\newlabel{tab:weak}{{2.1}{43}{The grid dimensions we used as we increased the number of MPI tasks. Since we are working with squared grids only, it is difficult to obtain exactly the same workload. However, as we can see, the workload remains the same up to a negligible difference}{table.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces  Weak efficiency for THIN nodes as we simultaneously increase the number of MPI tasks and the grid size, keeping the workload per process constant.}}{43}{figure.2.18}\protected@file@percent }
\newlabel{fig:weakmpithinhybrid}{{2.18}{43}{Weak efficiency for THIN nodes as we simultaneously increase the number of MPI tasks and the grid size, keeping the workload per process constant}{figure.2.18}{}}
\abx@aux@cite{0}{MPIIOtalk}
\abx@aux@segm{0}{0}{MPIIOtalk}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces  Weak efficiency for EPYC nodes as we simultaneously increase the number of MPI tasks and the grid size, keeping the workload per process constant.}}{44}{figure.2.19}\protected@file@percent }
\newlabel{fig:weakmpiepychybrid}{{2.19}{44}{Weak efficiency for EPYC nodes as we simultaneously increase the number of MPI tasks and the grid size, keeping the workload per process constant}{figure.2.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Conclusions}{44}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Optional - Results and Discussion: Algorithm 2}{44}{section.2.7}\protected@file@percent }
\abx@aux@cite{0}{glider}
\abx@aux@segm{0}{0}{glider}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}Serial comparison}{45}{subsection.2.7.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Comparison of serial versions for both algorithms on a randomly initialized grid and a sparse grid of size $10k$.}}{45}{table.2.2}\protected@file@percent }
\newlabel{tab:a2serial}{{2.2}{45}{Comparison of serial versions for both algorithms on a randomly initialized grid and a sparse grid of size $10k$}{table.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces  Time for A1 hybrid V2 implementation vs pure MPI implementation of A2, using a random grid and a sparse glider grid. When using a sparse grid, we obtain a considerable speedup.}}{46}{figure.2.20}\protected@file@percent }
\newlabel{fig:strongmpiepychybrid}{{2.20}{46}{Time for A1 hybrid V2 implementation vs pure MPI implementation of A2, using a random grid and a sparse glider grid. When using a sparse grid, we obtain a considerable speedup}{figure.2.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces  Speedup for strong MPI scalability for A1 hybrid V2 vs pure MPI implementation of A2. We use both a random grid and a sparse grid of size $10k$.}}{46}{figure.2.21}\protected@file@percent }
\newlabel{fig:strongmpiepychybridspeedup}{{2.21}{46}{Speedup for strong MPI scalability for A1 hybrid V2 vs pure MPI implementation of A2. We use both a random grid and a sparse grid of size $10k$}{figure.2.21}{}}
\abx@aux@read@bbl@mdfivesum{FD936CE143FE8B0CF7F76B5638A1881D}
\abx@aux@defaultrefcontext{0}{arch}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{conway}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{conway_patterns}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{pgm}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{mpi}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{omp}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{prace}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{glidergun}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{snarkloop}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{MPIIOtalk}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{conway_implementation}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{conway_slides}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{algo2}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{glider}{none/global//global/global}
\gdef \@abspage@last{49}
