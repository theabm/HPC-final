\documentclass{report}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[style=ieee, backend=biber]{biblatex}
\addbibresource{bibliography.bib}
\usepackage{csquotes}
\usepackage{float}

\title{High Performance Computing Project - A.A. 2022/2023}
\author{Andres Bermeo Marinelli}

\begin{document}
\maketitle
\tableofcontents
\chapter{Benchmarking MKL, OpenBLAS, and BLIS}

\section{Introduction}

In this exercise we compare the performance of three High Performance Libraries
(HPC): MKL, OpenBLAS, and BLIS. In particular, we focus on the level 3 BLAS 
function called \texttt{gemm}, which multiples an $m \times k$ matrix $A$ times 
a $k \times n$ matrix $B$ and stores the result in an $m \times n$ matrix $C$. 

This function comes in two types, one for single 
precision (float) and the other for double precision (double). Furthermore, it 
is capable of exploiting paralellism using OpenMP (OMP) to speed up the 
calculations, provided that we have the required computational resources.

Using squared matrices only, we perform a scalability study in two scenarios. 
In the first scenario, we fix the number of cores, and increase the size of the
matrices from $2000$ to $20000$. In the second scenario, we fix the matrix size 
to $10000$ and increase the number of cores that \texttt{gemm} can use by 
modifying the \texttt{OMP\_NUM\_THREADS} environment variable.

In both scenarios, we repeat the measurements for both single and double
precision, for both THIN and EPYC nodes, using the maximum number of cores.

Furthermore, for the second scenario, we also modify the thread affinity policy 
of OMP in order to observe any differences.

\section{Methodology}

\subsection{Compiling BLIS and obtaining binaries}

We begin by downloading the BLIS library by using the following commands:
\begin{verbatim}
    $git clone https://github.com/flame/blis.git
    $cd blis
    $srun -p {NODE} -n1 ./configure --enable-cblas --enable-threading=openmp --prefix=/path/to/myblis/lib auto
    $srun -p {NODE} -n 1 --cpus-per-task={P} make -j {P}
    $make install
\end{verbatim}

Where \texttt{NODE} can be specified as either \texttt{THIN} or \texttt{EPYC} and 
\texttt{P} are the available cores for each node, $24$ and $128$ respectively.

With these commands, we have compiled the BLIS library for the desired 
architecture.

Next, we specify the flag in the Makefile to compile for float or double using 
\texttt{\-DUSE\_FLOAT} or \texttt{\-DUSE\_DOUBLE}. Then, we run:
\begin{verbatim}
    $salloc -n {P} -N1 -p {NODE} --time=1:0:0
    $module load mkl/latest
    $module load openBLAS/0.3.23-omp
    $export LD_LIBRARY_PATH=/path/to/myblis/lib:$LD_LIBRARY_PATH
    $srun -n1 make cpu
\end{verbatim}

Which will generate the binaries for the desired architecture, with floar or 
double precision, depending on the flag we used.

To run, we use: 
\begin{verbatim}
    $srun -n1 --cpus-per-task=128  ./gemm_mkl.x {size_M} {size_K} {size_N}
    $srun -n1 --cpus-per-task=128  ./gemm_oblas.x {size_M} {size_K} {size_N}
    $srun -n1 --cpus-per-task=128  ./gemm_blis.x {size_M} {size_K} {size_N}
\end{verbatim}

At the end of this procedure, we should have the appropriate binaries for each 
architecture, and for each type of precision, double or float.

We now detail the steps to obtain the measurements for both scenarios.

\subsection{Using a fixed number of cores}

For this section, we use all the cores available in a THIN or an EPYC node: 24 
and 128, respectively. 

Since we only use squared matrices, we can describe the dimensions of the matrices 
with a single number, which we call "size". 

For both architectures, we start with a size of $2000$ and end with a size of 
$20000$, with jumps of $2000$ for a total of $10$ sizes. For each size, 
we repeat the measurement $10$ times and report the average and standard 
deviation.

Finally, we repeat the measurements for both floating point precision and double 
point precision.

The scripts that were used can be found in the folder \texttt{exercise2/scripts}, 
under the name \texttt{es2\_1\_thin.sh} and \texttt{es2\_1\_epyc.sh}.

It is important to observe that in this section, since we are using the entire 
node, there is little possibility to play with combinations of thread affinity.

This will be done for the next section.

Furthermore, contrary to the guidelines for the exercise, we decided to use the 
entire node to benchmark its full capacity, and also to avoid wasting 
resources. 

In fact, to obtain an accurate benchmark, we need to reserve the whole node, 
regardless of the number of cores we decide to use. This is because if other 
people began to use the other half of the node, this could introduce additional 
workloads which interfere with the benchmark. 

\subsection{Using a fixed matrix size}

For this section, we fix the size of the matrices to $10000$. Then, we slowly 
increase the number of cores to be used, until we reach the maximum. 

To set the number of cores, we change the environment variable 
\texttt{OMP\_NUM\_THREADS} to the desired value.

For THIN nodes, which have $24$ cores, we start using $1$ core, then $2$ and 
then we increase by steps of $2$, for a total of 13 points.

For EPYC nodes, which have $128$ cores, we start from $1$, then $10$ and then 
we increase by steps of $10$ until $120$. We also use $128$ cores, to see what 
happens at full capacity. We obtain a total of $14$ points.

We repeat all measurements $10$ times and report the average and standard 
deviation.

As usual, we repeat this process for both floating and double point precision.

In this section, we have the liberty to explore different thread allocation 
policies since we are not always using the whole node. 

We decided to use following combinations:
\begin{enumerate}
    \item \texttt{OMP\_PLACES=cores} and \texttt{OMP\_PROC\_BIND=close} 
    \item \texttt{OMP\_PLACES=cores} and \texttt{OMP\_PROC\_BIND=spread} 
\end{enumerate}

The scripts that were used can be found in the folder \texttt{exercise2/scripts}, 
under the names \texttt{es2\_2\_close\_thin.sh}, \texttt{es2\_2\_close\_epyc.sh}, 
\texttt{es2\_2\_spread\_thin.sh}, and \texttt{es2\_2\_spread\_epyc.sh}.

\section{Results and Discussion}

Before we discuss the results of both exercises individually, we briefly 
introduce the equation to calculate the theoretical peak performance ($T_{pp}$) 
of a machine:

\begin{equation}\label{eq:perf}
    T_{pp} = \text{Core Count}\times \text{clock freq.} \times \text{IPC}
\end{equation}

Where \texttt{IPC} is the instructions per cycle that the architecture 
is capable of executing.

This equation is very intuitive. The clock frequency tells us how many cycles 
per second a single core is able to achieve. The ipc factor tells us how many 
instructions per cycle the core can execute. This number is different for 
single precision (SP) and double precision (DP) operations. Finally, we need 
to multiple this by the number of cores that our machine has. 

On orfeo, THIN and EPYC nodes are composed of: 
\begin{itemize}
    \item THIN: $24$ Intel(R) Xeon(R) Gold $6126$ CPU's at $2.60$GHz - Skylake
    \item EPYC: $128$ EPYC AMD 7H12 CPU's at $2.60$GHz - Zen 2 (7002 a.k.a "Rome")
\end{itemize}

Skylake architecture is reported\cite{arch} to be able to execute $64$ SP FLOP per cycle 
and $32$ DP FLOP per cycle. On the other hand, Zen 2 is reported\cite{arch} to execute 
$32$ SP FLOP per cycle and $16$ DP FLOP per cycle.

Therefore, we obtain: 

\begin{table}[H]
\centering
\begin{tabular}{c|c|c|c|c|c|}
    Node Type & Total Cores & IPC (SP) & IPC (DP) & $T_{pp}$ (SP) & $T_{pp}$ (DP)\\\hline
    THIN      &     24      &    64    &   32     & $\sim 4$ TFLOPS & $\sim 2$ TFLOPS \\
    EPYC      &    128      &    32    &   16     & $\sim 10.6$ TFLOPS & $\sim 5.3$ TFLOPS \\
\end{tabular}
\caption{\label{tab:perf}Performance table of THIN and EPYC node architectures.}
\end{table}

Now we can proceed to discuss the results of the exercise.

\subsection{Using a fixed number of cores}

As mentioned above, in this section we keep the number of cores fixed to the 
maximum available in the node, namely 24 for THIN and 128 for EPYC, and we 
slowly increase the size of the matrices being multiplied from $m=2000$ to 
$m=20000$. 

We first show the results for THIN and then for EPYC nodes.

\subsubsection{THIN Nodes}

\begin{figure}[H]
% \centering
\hspace*{-2.5cm}
\includegraphics[width=10cm, height=6cm]{./images/fixed_cores_thin_float_gflops.pdf}
\includegraphics[width=10cm, height=6cm]{./images/fixed_cores_thin_float_gflops_ratio.pdf}
\caption{\label{fig:fixed_cores_thin_float} Results of SP matrix-matrix multiplication 
for THIN nodes. \texttt{MKL} and \texttt{OpenBLAS} perform similarly, outperforming 
\texttt{BLIS} for all matrix sizes.}
\end{figure}

We see that both \texttt{MKL} and \texttt{OpenBLAS} are able to reach $\sim 3.2$ 
TFLOPS, which is around $\sim80\%$ of $T_{pp}$. On the other hand, the \texttt{BLIS}
library is not able to exploit the full potential of the machine, arriving only
to $\sim 2.4$ TFLOPS, which is $\sim60\%$ of $T_{pp}$.

Furthermore, looking at the ratio of peak performance on the right, we observe 
that for small matrix sizes, none of the libraries are able to fully exploit 
the theoretical peak performance of the machine. This is most likely because the 
problem size is so small, that the majority of cores are starving for data rather 
than crunching numbers. In fact, we are able to reach the best performance when 
dealing with matrices of size $20000$. 

\begin{figure}[H]
% \centering
\hspace*{-2.5cm}
\includegraphics[width=10cm, height=6cm]{./images/fixed_cores_thin_double_gflops.pdf}
\includegraphics[width=10cm, height=6cm]{./images/fixed_cores_thin_double_gflops_ratio.pdf}
\caption{\label{fig:fixed_cores_thin_double} Results of DP matrix-matrix multiplication 
for THIN nodes. \texttt{MKL} performs the best, slightly above \texttt{OpenBLAS}. 
Both outperform \texttt{BLIS} for all matrix sizes.}
\end{figure}

We see that \texttt{MKL} reaches $\sim1.6$ TFLOPS, 
while \texttt{OpenBLAS} is slightly lower, at $\sim1.5$ TFLOPS, which is $\sim80\%$ and 
$\sim75\%$ of $T_{pp}$, respectively. On the other hand, \texttt{BLIS} arrives to 
$\sim1.3$ TFLOPS, which is $\sim65\%$ of $T_{pp}$.

Furthermore, since double precision is a heavier computation compared to single 
precision, we see that the libraries perform much better than the their single precision 
counterparts.
For example, looking at the plot for single precision, for matrix size of $4000$, 
we obtain on average around $40\%$ of $T_{pp}$. On the other hand, for double precision, 
looking at the same size, we are already at $50\%$ of $T_{pp}$.
\\

For both SP and DP, and for all matrix size, we notice that \texttt{MKL} and 
\texttt{OpenBLAS} are better able to exploit the full potential of a THIN node 
compared to \texttt{BLIS}. Therefore, on THIN nodes, if we need to multiply 
two matrices, we should always use either \texttt{MKL} or \texttt{OpenBLAS} to 
get some more performance. To get the absolute best performance, it is preferable 
to use \texttt{MKL}.

These results shouldn't be surprising, considering 
that \texttt{MKL} is developed by Intel and THIN nodes are Intel-based.
Therefore, it is natural to expect that this library is very fine-tuned to 
their own architecture and is able to exploit the performance of their machines.

Lastly, we observe the impressive results achieved by \texttt{OpenBLAS} which 
is based on the original implementation of Kazushige Goto, and is able to achieve a similar performance to 
\texttt{MKL}, which is maintained by an entire corporation.

\subsubsection{EPYC Nodes}

Now we show the results on EPYC nodes, which have a $T_{pp}$ of $10.6$ TFLOPS 
for SP and $5.3$ TFLOPS for DP.
\\\\
We first show the results of matrix-matrix multiplication for single point 
precision.

\begin{figure}[H]
% \centering
\hspace*{-2.5cm}
\includegraphics[width=10cm, height=6cm]{./images/fixed_cores_epyc_float_gflops.pdf}
\includegraphics[width=10cm, height=6cm]{./images/fixed_cores_epyc_float_gflops_ratio.pdf}
\caption{\label{fig:fixed_cores_epyc_float} Results of SP matrix-matrix multiplication 
for EPYC nodes. We notice that asymptotically, \texttt{OpenBLAS} outperforms 
\texttt{MKL} and \texttt{BLIS}, while for small matrices, \texttt{OpenBLAS} 
performs the worst.}
\end{figure} 

In this case, none of the libraries are able to reach more than $18\%$ of $T_{pp}$.
This could be an indication that to properly exploit a full EPYC node, we need 
to multiply much bigger matrices.
\\

We also notice that for matrices of size $\leq 9000$, \texttt{MKL} and \texttt{BLIS} 
outperform \texttt{OpenBLAS}. Between sizes $9000$ and $12000$, \texttt{MKL} 
performs best and \texttt{OpenBLAS} begins outperform \texttt{BLIS}. 
For sizes $\geq 12000$, \texttt{OpenBLAS} performs the best.

\begin{figure}[H]
% \centering
\hspace*{-2.5cm}
\includegraphics[width=10cm, height=6cm]{./images/fixed_cores_epyc_double_gflops.pdf}
\includegraphics[width=10cm, height=6cm]{./images/fixed_cores_epyc_double_gflops_ratio.pdf}
\caption{\label{fig:fixed_cores_epyc_double} Results of DP matrix-matrix multiplication 
for EPYC nodes. \texttt{BLIS} outperforms \texttt{MKL} and \texttt{OpenBLAS} 
for all matrix sizes.}
\end{figure}

Again, we notice that none of the libraries are able to achieve more than $18\%$ 
of $T_{pp}$. However, in this case, \texttt{BLIS} outperforms the other two libraries 
for all matrix sizes. The next best performer is \texttt{OpenBLAS}, followed by 
\texttt{MKL}, which performs the worst.
\\

In conclusion, on EPYC nodes, it seems that for DP matrix-matrix multiplication, 
it is better to use \texttt{BLIS}, while for SP, it is very dependent on the 
size of the matrices. For large matrices, we should use \texttt{OpenBLAS}
while for smaller ones, we should use \texttt{MKL}. 
Furthermore, evidence suggests that to fully exploit and EPYC node, we need to 
deal with much bigger matrices. So if we are multiplying matrices of size up to 
$20000$, it is much more convenient to just use a THIN node to get more 
performance.

\subsection{Using a fixed matrix size}

In this section, we fix the matrix size to $10000$ and we slowly increase the 
amount of cores that the libraries can exploit for multithreading through OMP. 
For THIN nodes, we arrive to $24$ cores, while for EPYC nodes, we arrive to 
$128$ cores.

Furthermore, since we are slowly increasing the number of cores that the libraries 
can use for multithreading, we can study the effects of using different 
thread allocation policies. In particular, we chose to use 
\texttt{OMP\_PROC\_BIND=close} and \texttt{OMP\_PROC\_BIND=spread}, while 
always using \texttt{OMP\_PLACES=cores}. 

In the first case, the threads will slowly occupy first one entire socket, and 
then, when it is full, the other one. In the second case, the threads will be 
placed as spread apart as possible, most likely on different sockets. 
In both cases, when we use the full node, we expect the results to be the same.

Furthermore, in contrast to the previous part of the exercise, we compare the 
GFLOPS obtained with the $T_{pp}$ calculated with the cores that are being used. 
In other words, in equation\ref{eq:perf}, instead of using the full 24 or 128 
to calculate $T_{pp}$ for the whole node, we use the number of cores we are using 
for that calculation.

We first analyze THIN and then EPYC nodes.

\subsubsection{THIN Nodes}
\begin{figure}[H]
% \centering
\hspace*{-2.5cm}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_thin_float_gflops_close.pdf}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_thin_float_gflops_close_ratio.pdf}
\caption{\label{fig:fixed_size_thin_float_close} Results of SP matrix-matrix 
    multiplication as the number of cores increase, using OMP close policy. 
    \texttt{MKL} and \texttt{OpenBLAS} obtain the best performance.}
\end{figure}

As we can see from the graph, both \texttt{MKL} and \texttt{OpenBLAS} are able 
to maintain $\sim 75\%$ of $T_{pp}$ for cores $\geq 6$. On the other 
hand, \texttt{BLIS} is able to achieve $\sim 60\%$ of $T_{pp}$. With $24$ cores 
this figure decreases to $55\%$. 

Interestingly, we notice that with $2$ cores, there is a significant performance 
drop. This is most likely explained by the fact that both cores are mapped to the 
same socket due to the close policy and must share resources such as the higher 
level caches, causing some contention.

Furtermore, once again, we notice that the best performing library is 
\texttt{MKL} which is Intel-based, closely followed by \texttt{OpenBLAS}.
\\

Now we analyze the results using a spread policy.

\begin{figure}[H]
% \centering
\hspace*{-2.5cm}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_thin_float_gflops_spread.pdf}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_thin_float_gflops_spread_ratio.pdf}
\caption{\label{fig:fixed_size_thin_float_spread} Results of SP matrix-matrix multiplication 
    as the number of cores increase, using OMP spread policy. \texttt{MKL} 
and \texttt{OpenBLAS} are the best perfomers.}
\end{figure}

Using a spread policy, we obtain very similar results to the case where we use 
a close policy. The main difference is that we don't have the same performance 
drop at 2 cores. This is most likely due to the fact that with a spread policy, 
each core is mapped to its own socket and there is no contention for resources. 

In fact, we notice that on average, the performance is slightly better than the 
close policy counterpart, and this is probably due to better resource usage 
from the beginning, since threads don't have to compete for resources immediately.
This highlights the importance of using the correct mapping policy to obtain 
better performance.
\\

Now we briefly analyze the results obtained for double precision.
\begin{figure}[H]
% \centering
\hspace*{-2.5cm}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_thin_double_gflops_close.pdf}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_thin_double_gflops_close_ratio.pdf}
\caption{\label{fig:fixed_size_thin_double_close} Results of DP matrix-matrix multiplication 
    as the number of cores increase, using close policy. \texttt{MKL} and \texttt{OpenBLAS}
perform the best.} 
\end{figure}

Similarly to the case of single precision, \texttt{MKL} is able to achieve around 
$\sim 75\%$ of $T_{pp}$. However, \texttt{OpenBLAS} suffers from a bit of performance 
degradation compared to the SP case. 

Once again, we notice the immediate drop in performance as soon as we use 2 cores, 
which is probably caused by the close policy. 

\begin{figure}[H]
% \centering
\hspace*{-2.5cm}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_thin_double_gflops_spread.pdf}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_thin_double_gflops_spread_ratio.pdf}
\caption{\label{fig:fixed_size_thin_double_spread} Results of DP matrix-matrix multiplication 
as the number of cores increase, using spread policy.}
\end{figure}

The analysis and discussion is very similar to the case of single point precision. 
We no longer see the drop at 2 cores, which is due to better resource usage from 
the beginning. Compared to the close policy, there is a slightly better performance 
for this scenario.

As usual, \texttt{MKL}, which is Intel-based, performs the best, 
closely followed by \texttt{OpenBLAS}, and finally, \texttt{BLIS}, which performs the worst. 

Finally, we analyze EPYC nodes.

\subsubsection{EPYC Nodes}

Considering the results obtained in the first part of the exercise, we expect 
that \texttt{MKL} won't be the dominating library anymore since we are using 
an AMD architecture. We also expect some more fluctuation in performance among 
the libraries, similarly to how there was a dependency on the matrix size. 

Furthermore, since we obtained low performance with matrices up to $20000$, using 
all 128 cores, we don't expect an asymptotic improvement. What may happen however,
is that when we use less cores, the performance will be much better compared 
to the theoretical peak performance (per number of cores this time).

We begin by analyzing the SP case with close policy.

\begin{figure}[H]
% \centering
\hspace*{-2.5cm}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_epyc_float_gflops_close.pdf}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_epyc_float_gflops_close_ratio.pdf}
\caption{\label{fig:fixed_size_epyc_float_close} Results of SP matrix-matrix multiplication 
as the number of cores increase, using close policy. The best performance is obtained 
by \texttt{OpenBLAS} with 40 cores.}
\end{figure}

Looking at the graph, we see that the best absolute performance is obtained by 
\texttt{OpenBLAS} at $40$ cores, however, this is only $\sim60\%$ of $T_{pp}$. 
On the other hand, for $10-20$ cores, the performance almost on part with $T_{pp}$. 
This tells us that to fully exploit the machine as intended, with a matrices of size 
$10000$, we only need a handful of cores.

This information is consistent with our previous hypothesis that to fully exploit 
an EPYC node, we need to consider much larger matrices. 

An immediate consequence of this observation, is that as we use more and more cores, 
the performance deteriorates considerably. When we use the full node, the 
performance is $\sim20\%$ of $T_{pp}$, which is higher than what we obtained for 
the first part of the exercise.

Contrary to what happens in THIN, we do not notice any severe drops in performance 
which could be due to the close policy mapping. However, this will be more 
noticeable once we analyze what happens when we use the spread policy.

Now we analyze the SP case using spread policy.

\begin{figure}[H]
% \centering
\hspace*{-2.5cm}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_epyc_float_gflops_spread.pdf}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_epyc_float_gflops_spread_ratio.pdf}
\caption{\label{fig:fixed_size_epyc_float_spread} Results of SP matrix-matrix multiplication 
as the number of cores increase, using spread policy.}
\end{figure}

There is a big change. The first thing is that now \texttt{BLIS} obtains the best 
performance, although it suffers from considerable fluctuations. Furthermore, 
\texttt{OpenBLAS}, which used to perform the best, now performs the worst out of 
all three libraries.

We also notice that using the spread policy causes both \texttt{MKL} and \texttt{BLIS}
to improve and \texttt{OpenBLAS} to worsen. This is interesting because it seems 
to suggest that the latter library is better able to handle resource contention 
while the first two are better at fully exploiting resources.

Once again, we notice that as we use more cores, we get worse performance.
\\
Now we look at the DP case with close policy.

\begin{figure}[H]
% \centering
\hspace*{-2.5cm}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_epyc_double_gflops_close.pdf}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_epyc_double_gflops_close_ratio.pdf}
\caption{\label{fig:fixed_size_epyc_double_close} Results of DP matrix-matrix multiplication 
as the number of cores increase, using close policy. The best performance is 
obtained by \texttt{BLIS}, followed by \texttt{OpenBLAS}.}
\end{figure}

For double precision, using a close policy, we observe that \texttt{BLIS} 
performs the best for many cores, while for the first 64 cores, it performs close 
to \texttt{OpenBLAS}, which is the best. \texttt{MKL} peforms the worst.
\\

Finally, we analyze the DP case with spread policy.

\begin{figure}[H]
% \centering
\hspace*{-2.5cm}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_epyc_double_gflops_spread.pdf}
\includegraphics[width=10cm, height=6cm]{./images/fixed_size_epyc_double_gflops_spread_ratio.pdf}
\caption{\label{fig:fixed_size_epyc_double_spread} Results of DP matrix-matrix multiplication 
as the number of cores increase, using spread policy.}
\end{figure}

Using a spread policy makes the gap between \texttt{BLIS} and \texttt{OpenBLAS} much 
wider. Also, \texttt{BLIS} seems to be more capable at maintaining the throughput 
as the number of cores are increasing. Of course, this means however, that the 
peformance compared to the theoretical peak is deteriorating.

\section{Conclusion}

After analyzing the results, we can state many conclusions. The first and most 
obvious one, is that it is better to use Intel-based implementations for Intel 
machines. This is evidenced by the fact that \texttt{MKL} consistently performed
the best in all scenarios for THIN, although it was closely followed by 
\texttt{OpenBLAS}. 

Next, if we are dealing with non-Intel architecures, such as EPYC, it is not 
advisable to use \texttt{MKL}, as it consistenly underperforms compared to 
\texttt{OpenBLAS} and \texttt{BLIS}. We also notice that \texttt{OpenBLAS} and 
\texttt{BLIS} behave completely differently, depending on the mapping policy we 
use.

We find that for matrices up to $20000$, THIN nodes are able to achieve 
$\sim 75\%$ of $T_{pp}$, with \texttt{MKL} and \texttt{OpenBLAS}. However, 
such matrix dimensions are to small to be able to fully exploit an EPYC node 
which has 128 cores. In fact, in all of our experiments, we are unable to obtain 
more than $20\%$ of the $T_{pp}$ of the full node.

Finally, as an improvement to this exercise, it would be very interesting to 
analyze what happens with an EPYC node, using matrices of size $50000$ or bigger 
perhaps, to understand whether we can obtain better performance in this case.

\chapter{Conway's Game of Life}


\section{Introduction}

This exercise is devoted to implementing a scalable version of Conway's Game of 
life\cite{conway}. The game consists of a $k\times k$ grid, where each cell can 
be "alive" or "dead".

The grid evolves over time by looking at a cell's $\mathcal{C}$ eight nearest 
neighbors and observing the following simple rules: 

\begin{itemize}
    \item A dead cell with exactly three live neighbors comes to live (\textit{birth}).
    \item A live cell with two or three neighbors stays alive (\textit{survival}).
    \item A dead or live cell less than two or more than three neighbors dies 
        or stays dead by under or overpolulation, respectively (\textit{death}).
\end{itemize}

These seemingly simple rules give rise to many interesting behaviors and patterns.

Depending on how we update the cells, there exist two methods to evolve the 
grid: \texttt{static} and \texttt{ordered}. In \texttt{static} evolution, we 
freeze the state of the grid $\mathcal{G}_t$ at time step $t$, and compute 
$\mathcal{G}_{t+1}$ separately, while looking at $\mathcal{G}_t$.

On the other hand, in \texttt{ordered} evolution, we start from a specific cell, 
usually in position $(0,0)$ (top left), and update the elements in\-place. 
In this scenario, the state of each cell depends on the evolution of all the 
cells before it. 

Our implementation must satisfy the following requirements: 

\begin{enumerate}
    \item Randomly initialize a square grid ("playground") of size $k \times k$ 
        with $k \geq 100$ and save it as a binary PGM file.
    \item Load a binary PGM file and evolve for $n$ steps.
    \item Save a snapshot during the course of evolution with frequency $s$ ($s=0$ 
        means save at the end). 
    \item Support both \texttt{static} and \texttt{ordered} evolution.
\end{enumerate}

Lastly, it must use both MPI\cite{mpi} and OpenMP\cite{omp} to paralellize 
the computations and be able to process grids of considerably high dimensions. 

\section{Methodology}

Since programs in MPI need to be rewritten completely from their serial 
counterparts, we must begin to conceptualize the problem in an encapsulated 
manner from the start.

At a high abstract level, we must make two important choices: 

\begin{enumerate}
    \item How we will decompose the problem. 
    \item How we will perform the IO (this has important consequences on the 
        organizational paradigm we will use).
\end{enumerate}

We briefly discuss both of these topics more in depth in the following subsections.

\subsection{Problem decomposition}

To exploit paralellism, we must first identify the concurrency in our 
application and then apply some form of decomposition. The two most important 
types of decomposition are \texttt{domain} and \texttt{functional} decomposition. 

In domain decomposition, multiple workers are performing the same set of 
instructions on different portions of data (SIMD). In functional 
decomposition, workers are performing different instructions on possibly 
different data (MIMD). 
 
In the case of static evolution, each cell can be updated independently from each 
other, as long as we have acccess to its neighbors. Therefore, one immediately 
obviously form of paralellism is for each MPI process to update their "part" 
of the whole grid. 

For this, we need to decide how to split the grid since we can do both a 1D or 
2D decomposition of our grid. It is known that a 2D decomposition is more 
efficient as it can exploit more bandwidth (more workers will send shorter messages 
at the same time). However, it is more complicated both from an implementation 
point of view, as well as it is worse for memory access. 

Let's talk briefly about this second aspect. Although we will talk about our 
grid as a 2D array, internally, for efficiency, it will be represented as a 1D 
array. Therefore, each process will work on a strip of continuous data as 
shown in the figure below. 

\begin{figure}[H]
\centering
\includegraphics[width=10cm, height=5cm]{./other_images/arraydecomposition.jpg}
\caption{\label{fig:decomposition}.}
\end{figure}


Fas one long array, in a 1D split, each process will need to work on a continuous


"strip" of this memory. 



\section{Implementation}

\section{Results and Discussion}

\section{Conclusions}

\subsection{How to add Tables}

% Use the table and tabular environments for basic tables --- see Table~\ref{tab:widgets}, for example. For more information, please see this help article on \href{https://www.overleaf.com/learn/latex/tables}{tables}. 
%
% \begin{table}
% \centering
% \begin{tabular}{l|r}
% Item & Quantity \\\hline
% Widgets & 42 \\
% Gadgets & 13
% \end{tabular}
% \caption{\label{tab:widgets}An example table.}
% \end{table}
%
% \subsection{How to add Comments and Track Changes}
%
% Comments can be added to your project by highlighting some text and clicking ``Add comment'' in the top right of the editor pane. To view existing comments, click on the Review menu in the toolbar above. To reply to a comment, click on the Reply button in the lower right corner of the comment. You can close the Review pane by clicking its name on the toolbar when you're done reviewing for the time being.
%
% Track changes are available on all our \href{https://www.overleaf.com/user/subscription/plans}{premium plans}, and can be toggled on or off using the option at the top of the Review pane. Track changes allow you to keep track of every change made to the document, along with the person making the change. 
%
% \subsection{How to add Lists}
%
% You can make lists with automatic numbering \dots
%
% \begin{enumerate}
% \item Like this,
% \item and like this.
% \end{enumerate}
% \dots or bullet points \dots
% \begin{itemize}
% \item Like this,
% \item and like this.
% \end{itemize}
%
% \subsection{How to write Mathematics}
%
% \LaTeX{} is great at typesetting mathematics. Let $X_1, X_2, \ldots, X_n$ be a sequence of independent and identically distributed random variables with $\text{E}[X_i] = \mu$ and $\text{Var}[X_i] = \sigma^2 < \infty$, and let
% \[S_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
%       = \frac{1}{n}\sum_{i}^{n} X_i\]
% denote their mean. Then as $n$ approaches infinity, the random variables $\sqrt{n}(S_n - \mu)$ converge in distribution to a normal $\mathcal{N}(0, \sigma^2)$.
%
%
% \subsection{How to change the margins and paper size}
%
% Usually the template you're using will have the page margins and paper size set correctly for that use-case. For example, if you're using a journal article template provided by the journal publisher, that template will be formatted according to their requirements. In these cases, it's best not to alter the margins directly.
%
% If however you're using a more general template, such as this one, and would like to alter the margins, a common way to do so is via the geometry package. You can find the geometry package loaded in the preamble at the top of this example file, and if you'd like to learn more about how to adjust the settings, please visit this help article on \href{https://www.overleaf.com/learn/latex/page_size_and_margins}{page size and margins}.
%
% \subsection{How to change the document language and spell check settings}
%
% Overleaf supports many different languages, including multiple different languages within one document. 
%
% To configure the document language, simply edit the option provided to the babel package in the preamble at the top of this example project. To learn more about the different options, please visit this help article on \href{https://www.overleaf.com/learn/latex/International_language_support}{international language support}.
%
% To change the spell check language, simply open the Overleaf menu at the top left of the editor window, scroll down to the spell check setting, and adjust accordingly.
%
% \subsection{How to add Citations and a References List}
%
% You can simply upload a \verb|.bib| file containing your BibTeX entries, created with a tool such as JabRef. You can then cite entries from it, like this: \cite{greenwade93}. Just remember to specify a bibliography style, as well as the filename of the \verb|.bib|. You can find a \href{https://www.overleaf.com/help/97-how-to-include-a-bibliography-using-bibtex}{video tutorial here} to learn more about BibTeX.
%
% If you have an \href{https://www.overleaf.com/user/subscription/plans}{upgraded account}, you can also import your Mendeley or Zotero library directly as a \verb|.bib| file, via the upload menu in the file-tree.
%
% \subsection{Good luck!}
%
% We hope you find Overleaf useful, and do take a look at our \href{https://www.overleaf.com/learn}{help library} for more tutorials and user guides! Please also let us know if you have any feedback using the Contact Us link at the bottom of the Overleaf menu --- or use the contact form at \url{https://www.overleaf.com/contact}.

\printbibliography

\end{document}
